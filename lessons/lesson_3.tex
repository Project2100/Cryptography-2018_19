\mychapter{3}{Lesson 3} %181003

\newcommand{\Extm}{\textup{\textsf{Ext}}}
\newcommand{\Extf}{\textrm{\textup{Ext}}}
\newcommand{\statdist}{\ensuremath{\Delta_\textsc{s}}}
\newcommand{\sdtu}{\ensuremath{\Delta_\textsc{u}}}

\section{Randomness Extraction}

As just seen in both examples of secret and auhentic communications, it is worth observing that they share a critical common mechanism in the form of a preemptively shared key that is picked uniformly at random form the key space. A problem then arises in actually trying to construct something, like a program, that generates ``true'', totally random keys in a timely fashion. The goal is indeed to efficiently extract uniform randomness form things that are not necessarily uniformly random, or worse yet, deterministic.

Suppose to have an unknown coin $C$ that has some nontrivial probability $p$ to evaluate heads. There is a simple algorithm devised by John Von Neumannn that is capable of extracting a fair coin out of it, and it is known as the \emph{Von Neumann extractor}:

\begin{enumerate}
    \item Let $C \sim \berdist(p)$ be a random variable
    \item \label{enum:VNEsample} Sample $c_1 \pickUAR B$
    \item Sample $c_2 \pickUAR B$
    \item If $c_1 = c_2$ go to step \ref{enum:VNEsample}
    \item \label{enum:VNEreturn} Else output $b_1$:
\end{enumerate}

At a high level, this is a loop that in theory could run infinitely; by analyzing it more carefully, the probability of breaking out the loop on an iteration is the same as when the two coin flips $c_1$ and $c_2$ evaluate to different values, which in total is $2p(1 - p)$. There is indeed some probability that a given iteration leads to a new one, but on a complete run's point of view, since all coin flips are independent, the number of iterations that occur before exiting follow a  geometric distribution in $p$, thus the probability of an increased number of failures decrease exponentially.\footnote{Nevertheless, such probability may still be very high when the original coin has a very heavy bias.}

This extractor gives some reasonable hope for constructing a more generic randomness extractor. Before going forward though, it may be best to have a way to measure how much a given random variable is close to a uniform one:

\begin{definition}
    Let $X$ be a random variable from a given probability distribution. A measure of its uniformity is its \emph{min-entropy}, defined as a negative logarithm of the highest probability among all of its outcomes:\footnote{This is a form of the more general definition of R\'enyi entropy}
    \[
        H_{\infty}(X) = -\log(\max_{x \in \Omega}(\Pr[X = x]))
    \]
\end{definition}

To get a feel of this measure, it's worth starting from the trivial case of a ``constant'' random variable $X$ that certainly evaluates to an outcome $x$; by definition, the highest probability on the outcomes of $X$ is 1, therefore: 
\[
    H_{\infty}(X) = -\log(\Pr[X = \overline{x}]) = -\log(1) = 0
\]

Unsurprisingly, a constant variable is useless in creating a uniform distribution: it always gives the same outcome, it is effectively deterministic. On the other hand, let $Y$ be a uniform random variable over some finite outcome space $\Omega$; in this case, all probabilities are the same, thus:
\[
    \forall y \in \Omega \qquad H_{\infty}(Y) = -\log(\Pr[Y = y]) = -\log(|\Omega|^{-1}) = \log(|\Omega|)
\]

\begin{figure}
    \centering
    \begin{tikzpicture}
        \datavisualization[
                scientific axes,
                visualize as smooth line/.list = {first, second},
                x axis = {label = {$\Pr[C = 1]$}},
                y axis = {label = {$H_\infty(C)$}},
                data/format = function]
            data[set = first] {
                var x : interval[0 : 1/2];
                func y = - ln(1 - \value x);
            }
            data[set = second] {
                var x : interval[1/2 : 1];
                func y = - ln(\value x);
            }
        ;
    \end{tikzpicture}
    \caption{The min-entropy of a coin $C$ as a function of how likely it evaluates to heads}
    \label{fig:coinminentropy}
\end{figure}

Figure \ref{fig:coinminentropy} shows how the min-entropy of a coin $C$ variates by tuning its probability of evaluating to heads; it is apparent that the min-entropy reaches the highest point when the coin is fair (i.e. uniform), and goes to zero when it effectively behaves like a constant random variable.

%This point can be further expanded:
%\begin{proposition}
%     The min-entropy of any random variable $X$ is upper bounded by a specific function of its outcome space:
%     \[
%         \forall X \in \mathcal{R}and_\Omega \qquad H_\infty(X) \leq -\log(|\Omega|^{-1})
%     \]
%\end{proposition}
%\begin{proof}
%\end{proof}

Having some grasp on how the measure of min-entropy works, the problem of extracting uniform randomness from non-uniform sources can then be reformulated as the problem of maximizing min-entropy of a given random variable that, in the general case, is not maximal.

In order to have a simplified exposition, the following paragraphs will deal with binary strings of some fixed length $n$ as outcomes, and the definition of min-entropy will be adapted to employ a base-2 logarithm. In this framework, the min-entrpoy of a uniform random variable $U$ over the string space $\binary^n$ becomes the string length itself:\footnote{This also explains how the string length is a recurrent topic in cryptography discussions, as it indirectly expresses a cryptosystem's strength: the greater its min-entropy, the harder it is to find the right key for a ciphertext from scratch.}
\[
    H_\infty(X) = \log_2(|\binary^n|) = \log_2(2^n) = n
\]

Despite such efforts for constructing a universal extractor, a general counterexample can always be found:

\todo{ Lots of unsolved questions here:
\begin{itemize}
    \item Is it possible to put a bound on the min-entropy instead of an equality, and gain some added value out of this claim?
    \item Both the statement and the proof have some hideous caveats that may stem from confusing the extractor as a machine with the extractor as a function, and ultimately lead to a statement that blindly treats two RVs $X$ and $Y$ as "equivalent"
\end{itemize}
... actually, is a statement about min-entropy even needed?
}

\begin{claim}
    There is no universal algorithm $\Extm$ that, given a random variable $X$ with relatively high min-entropy:
    \[
        H_\infty(X) \leq n - 1
    \]
    returns a uniform random variable $U$:
    \[
        \Extm \in \binary \to \binary : \forall X \in \mathcal{R}and_{\binary} : H_\infty(X) = n - 1 \qquad \Extm(X) \sim U
    \]
\end{claim}

\begin{proof}
    Assume there exists such a universal extractor $\Extm$, and let $C$ be a coin with min-entropy $n - 1$. The general gist of $\Extm$ is that it uses $C$ multiple times, each time $i$ obtaining an outcome $q_i$, in order to generate the random output. Let $q$ be the generic sequence of outcomes that $\Extm$ receives; in this perspectve, it is possible to reformulate the extractor as a function $\Extf$, where its domain $Q$ is composed of the sequences $q$ for which $\Extm$ does stop and gives an output:\footnote{Not to be confused with the \emph{recognized language} of $\Extm$, as $Q$ also includes the sequences for which the extractor yieds 0}
    \[
        Q \subseteq \binary^* \qquad \Extf \in Q \to \binary : q \in Q \Longleftrightarrow \Extm \textrm{ stops}
    \]

    From there, $Q$ can be seen as bipartitioned by the set $Q_1$ that collects the sequences that evaluate to 1,\footnote{And that can be called the language recognized by $\Extf$} and $Q_0$ that collects the sequences evaluating to 0.

    \todo{And here's the supposed plot hole: $Y$ and $X$ cannot be in such a direct relationship, as $Y$ destroys the independencies of each toss of $X$}

    Let $Y$ be a random variable in $\binary^*$ such that it distributes uniformly over the greatest block between $Q_1$ and $Q_0$. Then the min-entropy of $Y$ is $\log(|Q_1|)$; however, since all the inputs in $Q_1$ are mapped to 1 by the extractor function:
    \[
        \forall q \notin Q_1 \qquad \Pr[Y = q] = 0 \implies \Pr[\Extf(Y) = 1] = 1
    \]

    which contradicts the property of the original $\Extm$ to be a universal extractor.
\end{proof}

% This is probably the original version, but that is really unrelated to some thing as the Von Neumann Extractor:
%
% Let Ext be a universal extractor from 2^n to 2, and X be a nontrivial RV in 2^n; then Ext(X) sim unifdist by definition.
% Since ext is a machine (determinitstic!), it can be equivalent to a math function, and in this setting, its domain can be split parts Q0 and Q1 according to Ext. Now let Y be a RV that uniformly distributes over the greatest of Q0 and Q1. Then, the minentroy of Y is > n-1, however Ext(Y) will always return the same value, i.e. it has become constant. Contradiction; thus ext cannot be universal

So there can be no construct that transforms any unknown distribution into the uniform one. This can be changed if, along with the unknown randomness, a smaller amount of uniform randomness is introduced to produce the desired uniform distribution. Before formalizing such idea, another notion of measure related to probability distributions can help:

\begin{definition}[Statistical distance]
    Let $X$ and $Y$ be two random variables on the same probability space. Then, for each outcome, take half the absolute difference of probabilities between the two, and sum all of them up. The result is called the \emph{statistical distance} between $X$ and $Y$:
    \[
        \statdist(X, Y) = \half\sum_{x \in \Omega}|\Pr[X = x] - \Pr[Y = x]|
    \]
\end{definition}

\begin{figure}
    \centering
    \begin{tikzpicture}
        \datavisualization[
                scientific axes = clean,
                visualize as scatter/.list = {first, second},
                style sheet = cross marks,
                first = {label in legend = {text = $X$}},
                second = {label in legend = {text = $Y$}},
                x axis = {label = {$\omega$}},
                y axis = {label = {$\Pr$}, include value = {0, 1}}]
            data[set = first] {
                x, y
                0, 0.25
                1, 0.15
                2, 0.35
                3, 0.05
                4, 0.2
            }
            data[set = second] {
                x, y
                0, 0.05
                1, 0.2
                2, 0.1
                3, 0.3
                4, 0.35
            }
        ;
    \end{tikzpicture}
    \caption{Two random distributions denoted espectively by random variables $X$ and $Y$}
    \label{fig:statdist}
\end{figure}

Figure \ref{fig:statdist} shows a visual interpretation of such measure with two arbitrary random variables, where their statistical distance is half the sum of the distances of the points on each column.

In most scenarios, given a random variable $X$, it is valuable to know how much it is distant to a uniform random variable $U$ over the same space $\Omega$. To this purpose, the notation $\statdist(X, U)$ will be shortened to $\sdtu(X)$, making any definition of uniform variables implicit.

This measure contributes in building a slightly more sophisticated definition of a randomness extractor:

\begin{definition}[Seeded extractor]
    Let $\Extm$ be a machine having two binary inputs of length $d$ and $n$, and binary output length $l$:\footnote{Spoiler: The first input, of length $d$, is also called a \emph{seed}}
    \[
        \Extm: \binary^d \times \binary^n \to \binary^l
    \]
    and let $S$ and be a uniform random variable in the irst input $\binary^d$. Then $\Extm$ is called a ($k$, $\varepsilon$)-extractor iff, for any random variable $X$ with min-entropy at least $k$, the output distribution of $\Extm(S, X)$ is statistically close to a uniform distribution within a bound of $\varepsilon$:
    \[
        \forall X : H_{\infty}(X) \geq k \implies \statdist((S, \Extm(S, X)), (S, \unifdist(\binary^l))) \leq \varepsilon
    \]
\end{definition}

% AP210312: Consider putting the conversion to SDTU here

\subsubsection{Universal hash functions}

The hash function families introduced earlier turn out to be a straightforward tool in constructing seeded extractors; they work even under weaker assumptions. For this, consider the probability notion of \emph{collision}:

\begin{definition}[Collision probability]
    Let $X$ and $Y$ be two \iid{} random variables; a \emph{collision} is the event of both variables evaluating to the same outcome. Such event probabilities are denoted as:
    \[
        Col(X) = Col(Y) = \Pr[X = Y]
    \]
\end{definition}

A different formulation of collision probability can be reached by simple manipulations, with the added benefit that it refers to only one of the two \iid{} random variables:
\begin{align*}
    \Pr(X = Y) =& \sum_{x \in \Omega} \Pr[X = x \wedge Y = x] & \text{(Total probability)} \\
               =& \sum_{x \in \Omega} \Pr[X = x] \Pr[Y = x]   & \text{(Independency)}      \\
               =& \sum_{x \in \Omega} \Pr[X = x]^2                                         \\
\end{align*}

\begin{definition}
    Let $S$ be a uniform seed. A hash function family $H$ is deemed \emph{universal} iff:
    \[
        \forall a \neq b \in \Omega \implies \Pr[h_S(a) = h_S(b)] = \oneover{\binary^l}
    \]
\end{definition}

\begin{lemma}[Collision bound] \label{lem:colbound}
    Let $X$ be a random variable such that its collision probability is upper bounded by the following function of some positive value $\varepsilon$ arbitrarily close to $0$:
    \[
        Col(X) \leq \frac{1 + 4\varepsilon^2}{|\Omega|}
    \]
    Then $\sdtu(X) \leq \varepsilon$, meaning that $X$ is almost uniform over $\Omega$.
\end{lemma}


\begin{proof}
    By definition of statistical distance:
    \[
        \sdtu(X) = \half \sum_{x \in \Omega} \left| \Pr[X = x] - \oneover{|\Omega|} \right|
    \]

    Decompose each of the above addends into the couple $q_x$ and $s_x$:
    \[
        q_x = \Pr[X = x] - \oneover{|\Omega|} \qquad s_x =
        \begin{cases}
            1  & \text{if $q_x \geq 0$} \\
            -1 & \text{otherwise}
        \end{cases}
    \]
    
    Then, by the Euclidean variant of the Cauchy-Schwarz inequality:
    \begin{align*}
        \sdtu(X) =&\ \half \sum_{x \in \Omega} q_x s_x                                                                & \text{(Decomposition)}     \\
              \leq&\ \half \sqrt{ \left( \sum_{x \in \Omega} q_x^2 \right) \left( \sum_{x \in \Omega} s_x^2 \right) } & \text{(Cauchy-Schwarz)}    \\
                 =&\ \half \sqrt{|\Omega| \sum_{x \in \Omega} q_x^2}                                                  & (\forall x \; (s_x^2 = 1)) \\
    \end{align*}

    Focusing on the sum $\sum_{x \in \Omega} q_x^2$:
    \begin{align*}
        \sum_{y \in \Omega} q_x^2 =&\ \sum_{x \in \Omega} \left( \Pr[X = x] - \oneover{|\Omega|} \right)^2                                   &                        \\
                                  =&\ \sum_{x \in \Omega} \left( \Pr[X = x]^2 - \frac{2}{|\Omega|} \Pr[X = x] + \oneover{|\Omega|^2} \right) &                        \\
                                  =&\ Col(X) - \frac{2}{|\Omega|} + \frac{1}{|\Omega|}                                                       &                        \\
                               \leq&\ \frac{1 + 4 \varepsilon^2}{|\Omega|} - \frac{2}{|\Omega|} + \oneover{|\Omega|}                         & \text{(By hypothesis)} \\
                                  =&\ \frac{4 \varepsilon^2}{|\Omega|}                                                                       &                        \\
    \end{align*}

    Thus, getting back to the statistical distance evaluation:
    \begin{align*}
        \sdtu(X) \leq&\ \half \sqrt{|\Omega| \sum_{x \in \Omega} q_x^2}        \\
                 \leq&\ \half \sqrt{|\Omega| \frac{4 \varepsilon^2}{|\Omega|}} \\
                    =&\ \half 2 \varepsilon = \varepsilon                      \\
    \end{align*}

    and by stating that $\varepsilon$ is arbitrarily close to zero, $X$ is statistically close to the uniform distribution over $\Omega$.
\end{proof}

\subsubsection{Leftover hash lemma}

This lemma has been proven by Russell Impagliazzo, Leonid Levin and Michael Luby:

\begin{lemma}[Leftover hash]
    Let $h_s$ be a pairwise-independent hash function with uniform seed $S$, and $\Extm \in \binary^d \times \binary^n \to \binary^l$ be a seeded randomness extractor. Then if $\Extm(S, x) = h_S(x)$, and $x$ is governed by a random variable $X$ with min-entropy $H_{\infty}(X) \geq k$, where:
    \[
       k = l - 2 \log_2 \varepsilon - 2
    \]
    then $\Extm$ is a ($k$, $\varepsilon$)-seeded randomness extractor.
\end{lemma}

\begin{proof}
    The extractor definition requires that:
    \[
        \statdist(Y, (S, U_l)) \leq \varepsilon
    \]
    where $U_l$ distributes uniformly over $\binary^l$. Although the appearance of $S$ in both operands may seem to complicate things, the distance formulation can be changed to remove such constraint, and become simpler:
    \begin{align*}
         &\ \statdist((S, h_S(X))), (S, U_l))                                                                                               &                \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \Pr[(S, U_l) = (s, t)] \right|                      &                \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \oneover{|\binary|^d} \oneover{|\binary^l|} \right| & (S \indep U_l) \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \Pr[U_{d + l} = (s, t)] \right|                     &                \\
         &                                                                                          & \mathllap{(U_{d + l} \sim \unifdist(\binary^{d + l}))} \\
        =&\ \statdist((S, h_S(X)), U_{d + l})                                                                                               &                \\
        =&\ \sdtu((S, h_S(X)))                                                                                                              &                \\
    \end{align*}
    
    At this point the \hyperref[lem:colbound]{collision bound} can be used to put an upper bound on this statistical distance. To this end, let $Y = (S, h_S(X))$ and $Y = (S, h_S(X))$ be two \iid{} random variables; the collision probability of $Y$ equates to:
    \begin{align*}
         &\ Col(Y)                                               &                                               \\
        =&\ \Pr[Y = Y']                                          &                                               \\
        =&\ \Pr[S = S' \wedge h_S(X) = h_{S'}(X')]               & \mathllap{\text{(By definition)}}             \\
        =&\ \Pr[S = S'] \Pr[h_S(X) = h_{S'}(X') \knowing S = S'] & \mathllap{\text{(Conditional prob.)}}         \\
        =&\ \Pr[S = S'] \Pr[h_S(X) = h_S(X')]                    & \mathllap{\text{(Condition collapse)}}        \\
        =&\ 2^{-d} \Pr[h_S(X) = h_S(X')]                         & \mathllap{\text{(Uniform collision prob.)}}   \\
        =&\ 2^{-d} \left( \Pr[h_S(X) = h_S(X') \wedge X = X'] + \Pr[h_S(X) = h_S(X') \wedge X \neq X'] \right) & \\
         &                                                       & \mathllap{\text{(Total probability)}}         \\
    \end{align*}

    The two addends are tackled separately. For the first one: 

    \begin{align*}
         &\ \Pr[h_S(X) = h_S(X') \wedge X = X']               &                               \\
        =&\ \Pr[X = X'] \Pr[h_S(X) = h_S(X') \knowing X = X'] & \text{(Conditional prob.)}    \\
        =&\ Col(X) \Pr[h_S(X) = h_S(X') \knowing X = X']      & \text{(Collision def.)}       \\
        =&\ Col(X) \Pr[h_S(X) = h_S(X)]                       & \text{(Condition collapse)}   \\
        =&\ Col(X)                                            & \text{(Prob. of a tautology)} \\
    \end{align*}

    \begin{proposition}
        Given any random variable $X$:
        \[
            H_\infty(X) \geq k \implies Col(X) \leq 2^{-k} \qedhere
        \]
    \end{proposition}

    \begin{proof}
        By definition of min-entropy\footnote{The definition specifies explicitly a base-2 logarithm, but can actually be any base; in case, the statement to be proved shall correct the collision probability bound accordingly by changing its exponential basis}:
        \begin{align*}
            -\log \max_{x \in \Omega}(\Pr [X = x]) &\geq k      \\
            \log \max_{x \in \Omega}(\Pr [X = x])  &\leq -k     \\
            \max_{x \in \Omega}(\Pr [X = x])       &\leq 2^{-k} \\
        \end{align*}

        On the other hand:
        \begin{align*}
            Col(X) =&\ \sum_{x \in \Omega} \Pr[X = x]^2                               \\
                \leq&\ \sum_{x \in \Omega} \Pr[X = x] \max_{y \in \Omega}(\Pr[X = y]) \\
                   =&\ \max_{y \in \Omega}(\Pr[X = y]) \sum_{x \in \Omega} \Pr[X = x] \\
                   =&\ \max_{y \in \Omega}(\Pr[X = y])                                \\
        \end{align*}

        Therefore:
        \[
            Col(X) \leq \max_{x \in \Omega}(\Pr[X = x]) \leq 2^{-k}
        \]
    \end{proof}

    Shifting the focus to the second addend:

    \begin{align*}
            &\ \Pr[h_S(X) = h_S(X') \wedge X \neq X']                  &                                                  \\
           =&\ \Pr[X \neq X'] \Pr[h_S(X) = h_S(X') \knowing X \neq X'] & \text{(Conditional prob.)}                       \\
        \leq&\ \Pr[h_S(X) = h_S(X') \knowing X \neq X']                & \mathllap{\text{(Prob. is not greater than 1)}}  \\
    \end{align*}

    \begin{proposition}
        If $h_S$ is a pairwise independent hash function, then:
        \[
            \Pr[h_S(X) = h_S(X') \knowing X \neq X'] \leq 2^{-l}
        \]
    \end{proposition}
    
    \begin{proof}
        \todo{Left as an exercise.}
    \end{proof}

    Going back to the collision probability of $Y$, and using the two propositions above:
    \begin{align*}
        Col(Y) \leq&\ 2^{-d} \left( Col(X) + \Pr[h_S(X) = h_S(X') \knowing X \neq X'] \right) & \\
               \leq&\ 2^{-d} (2^{-k} + 2^{-l}) & \\
                  =&\ 2^{-d - l} (2^{-k + l} + 2^{-l + l}) & \\
                  =&\ 2^{-d - l} (2^{-(l - 2 \log_2(\varepsilon) - 2) + l} + 1)  & \\
                  =&\ 2^{-d - l} (2^{2 \log_2(\varepsilon) + 2} + 1)  & \\
                  =&\ 2^{-d - l} (4\varepsilon^2 + 1) & \\
    \end{align*}

    Applying the collision bound entails that $\sdtu(Y) \leq \varepsilon$, therefore $h_S$ is a $(k, \varepsilon)$-seeded randomness extractor.
\end{proof}

Note that for smaller values of $\varepsilon$ we have greater values of min-entropy. A problem that is still open is to find an extractor with $k \approx l$.
