\mychapter{3}{Lesson 3} %181003

\newcommand{\ext}{\textup{\textsf{Ext}}}
\newcommand{\statdist}{\ensuremath{\Delta_\textsc{s}}}
\newcommand{\sdtu}{\ensuremath{\Delta_\textsc{u}}}

\section{Randomness Extraction}

In most of our discourse, the subject of uniformly random variables is much recurrent; this chapter/lesson delves deeper into the topic. For starters, we devise some attempts to extract uniform randomness from ``non-uniform'' randomness sources.

Suppose to have a biased coin $B \sim \berdist(p) : p \neq \half$. How to craft a fair coin out of it? In his time, Von Neumann devised a simple algorithm, which is now known as the \emph{Von Neumann extractor}:

\begin{enumerate}
    \item Let $B \sim \berdist(p)$ be a random variable
    \item \label{enum:VNEsample} Sample $b_1 \pickUAR B$
    \item Sample $b_2 \pickUAR B$
    \item If $b_1 = b_2$ go to step \ref{enum:VNEsample}
    \item \label{enum:VNEreturn} Else:
    \begin{itemize}
        \item If $b_1 = 0 \wedge b_2 = 1$ output 1
        \item If $b_1 = 1 \wedge b_2 = 0$ output 0
    \end{itemize}
\end{enumerate}

Some considerations can be made: The probability of both single cases in step \ref{enum:VNEreturn} is $p(1 - p)$, therefore the probability to reach it is $2p(1 - p)$. Also, it is apparent that the number of possible failures in reaching step \ref{enum:VNEreturn} follow a geometric distribution in $p$, thus the probability of increased number of failures decrease exponentially.

We now get back to our ultimate goal. Let $X$ be any random variable over a space $\Omega$, we wish to design an ``extraction'' algorithm \ext{} such that $U = \ext(X)$ distributes uniformly over $\Omega$. To help ourselves, we will deal with probability spaces of binary strings ($\binary^n$), and define a measure of ``how much'' a distribution is uniform over its space:

\begin{definition} Let X be a random variable from a given probability distribution. Its \emph{min-entropy} is defined as follows:
\[
    H_{\infty}(X) = -\log_2(\max(\Pr[X = x]))
\]
\end{definition}

Using this measure, we can already see an interesting case, which involves ``constant'' random variables:
\begin{align*}
    X \sim \mathcal{C}onst(\overline{x}) \implies & \Pr[X = \overline{x}] = 1                                       \\
                                         \implies & \Pr[X \neq \overline{x}] = 0                                    \\
                                         \implies & H_{\infty}(X) = -\log_2(\Pr[X = \overline{x}]) = -\log_2(1) = 0 \\
\end{align*}

And in fact, a constant variable is useless in creating a uniform distribution: it always gives the same outcome, making everything deterministic. Therefore, such variables must be excluded in our search for a ``universal extractor''.
On the other hand, looking at a uniform distribution:
\begin{align*}
    X \sim \unifdist(\Omega) \implies & \forall x \Pr[X = x] = \oneover{|\Omega|}   \\
                             \implies & H_{\infty}(X) = -\log_2(\oneover{|\Omega|})
\end{align*}

Knowing that $\Omega$ is be our usual domain choice of binary strings of a given length $\binary^n$, the min-entropy becomes exactly $n$\footnote{This also sheds some light in how string length is a frequent topic in the cryptography realm, as it usually expresses a cryptosystem's strength: the greater its min-entropy, the harder it is to find the right key from scratch for a ciphertext.}.
Using this measure, we can actually seek how much min-entropy we require in the original distribution $X$ in order for the extractor to return a uniform distribution. Ideally, we would like a value as close to 0 as possible, because a min-entropy of zero leads to constant variables, which have been excluded beforehand. Alas, it turns out that:
\begin{claim}
    There is no such universal \ext{} algorithm that returns a uniform distribution from random variables $X$ with min-entropy $H_{\infty}(X) \leq n - 1$
\end{claim}

\begin{proof}
    % Notes: In our domain, the extraction problem to a unifdist reduces to an extraction of a fair coin; from there, creating a unifdist consists in doing as many coin flips as the strings' length in the domain of choice (remember we're still in the domain of bianry strings, each coin flip is essentially one bit).

    % Furthermore, extraction of a fair coin reduces to the extraction of a generic bernoulli variable: from there we can simply use the Von Neumann extractor to get a fair coin

    % WHAT'S HAPPENING DOWN HERE?!?!?!

    %Let \ext{} be a candidate extractor which outputs a fair coin from any given random variable $X$ in a fixed length binary string space. The resulting coin effectively splits $X$'s domain in two parts $X_0$ and $X_1$, in an attempt to balance the probability that $X$ is in either part. Now, pick the biggest one $X_b$

    %ILLUMINATION: fix ext , run with any X, ext bipartitions the domain, define Y to be unif over an arbitrary part b. Ext(Y) will be forced to output the constant RV on b. contradiction

    %AP190904: Don't like this model, appears to not reflect exactly the matter at hand; algorithms =/= functions

    % Let \ext{} be a candidate extractor, assume that $X$ is any random variable for which $\ext(X)$ is a fair coin. Going deeper into the model, we can figure the extraction result to fairly bipartition the "language" that is formed by repeated saplings from X, as in: "\Omega*". For example, the Von Neumann extractor recognizes the language (00|11)*(01)(0|1)* and doesn't recognize (00|11)*(10)(0|1)*.
    % 

    \todo{Help with the proof, things don't look good}

    %Let \ext{} be a candidate extractor, assume that $X$ is any random variable for which $\ext(X)$ is a fair coin. We are %in a situation where 
    %
    %Let \ext{} be a candidate extractor, Let b be any binary value s.t. $|Ext^-1(b)|$ is maximal ($Ext^-1: \binary \to \binary^n$)\\
    %$\implies |Ext^-1(b)|\geq 2^{n-1}=\frac{2^2}{2}$
    %
    %\begin{figure}[ht]
    %    \centering
    %    \begin{tikzpicture}[>=latex]
    %        
    %        \node (a1) {};
    %        \node[below=0.3cm of a1] (a2) {};
    %        \node[below=0.3cm of a2] (a3) {};
    %
    %        \node[below=0.6cm of a3] (a4) {};
    %        \node[below=0.3cm of a4] (a5) {};
    %        \node[below=0.3cm of a5] (a6) {};
    %
    %        \node[below=0.3cm of a3] (l) {};
    %        \node[right=1.4cm of l] (l1) {};
    %        \node[left=1.4cm of l] (l2) {};
    %        \draw[-,black] (l2) -- (l1);
    %
    %        \node[right=4cm of a2] (b1) {$x=0$};
    %        \node[right=4cm of a5] (b3) {$x=1$};
    %
    %        \node[ellipse,line width = 1pt, draw=black,minimum size=3cm,fit={(a1) (a6)}] {};
    %
    %        \node[below=1cm of a6,font=\color{black}\Large] {$\binary^n$};
    %
    %        \draw[-,black] (a3) to[out=-10,in=190] (b1.190);
    %        \draw[-,black] (a2) -- (b1.180);
    %        \draw[-,black] (a1) to[out=10,in=170] (b1.170);
    %
    %        \draw[-,black] (a6) to[out=-10,in=190] (b3.190);
    %        \draw[-,black] (a5) -- (b3.180);
    %        \draw[-,black] (a4) to[out=10,in=170] (b3.170);
    %    \end{tikzpicture}
    %\end{figure}
    %
    %Define $X$ to be uniform over $Ext^-1(b)$ so $H_{\infty}(x)=n-1$ but $Ext(X)=b$ CONSTANT.
    %
    %\dots

\end{proof}

So this approach is doomed, unless we factor in a preemptive small amount of true randomness in the algorithm. This is what a \emph{seeded extractor} does:
\[
    \ext: \underbrace{\binary^d}_{seed(public)}\times \underbrace{\binary^n}_{input} \to \underbrace{\binary^l}_{output}
\]

Before giving a formal definition of such an extractor, we require another notion of measure related to probability distributions:

\begin{definition} \emph{Statistical Distance}:
    Let $X$ and $Y$ be two random variables on the same probability space. Their \emph{statistical distance} is defined as follows:
    \[
        \statdist(X, Y) = \half\sum_{x \in \Omega}|\Pr[X = x] - \Pr[Y = x]|
    \]

\end{definition}

From a more visual perspective, this distance amounts to half the area delimited by the two distributions, if drawn one over another on the outcome space.

\todo{Image of the statistical distance}

In most scenarios, given a random variable $X$, it is valuable to know how much it is distant to a uniform random variable $U$ over the same space $\Omega$. To this purpose, the notation $\statdist(X, U)$ will be shortened to $\sdtu(X)$, making any definition of uniform variables implicit.

\begin{definition}
    Let $\ext \in \binary^d \times \binary^n \to \binary^l$ be a seeded extractor, and $S \sim \unifdist(\binary^d)$. Then it is a ($k$, $\varepsilon$)-extractor iff:
    \[
        \forall X : H_{\infty}(X) \geq k \implies \statdist((S, \ext(S, X)), (S, \unifdist(\binary^l))) \leq \varepsilon
    \]
\end{definition}

Do note that $S$ takes part in both sides of the statistical distance: this is to be interpreted that the seed is known at the time of extraction.

\todo{Need to rethink this definition further}

\subsection{Universal hash functions}

Getting back to our hash function families, we see that they too use an argument as a random seed, and attempt to be as uniform as possible; thus they behave in most ways as seeded extractors. Let's further develop the idea:

\begin{definition}
    Let $S$ be a uniform seed. A hash function family $H$ is deemed \emph{universal} iff:
    \[
        \forall a \neq b \in \Omega \implies \Pr[h_S(a) = h_S(b)] = \oneover{\binary^l}
    \]
\end{definition}

\begin{definition}
    Let $X$ and $Y$ be two \iid{} random variables; a \emph{collision} is the event of both variables evaluating to the same outcome. Such event probabilities are denoted as:
    \[
        Col(X) = Col(Y) = \Pr[X = Y]
    \]
\end{definition}

A different formulation of collision probability can be reached by simple manipulations, with the added benefit that it refers to only one of the two \iid{} random variables:
\begin{align*}
    \Pr(X = Y) =& \sum_{x \in \Omega} \Pr[X = x \wedge Y = x] & \text{(Total probability)} \\
               =& \sum_{x \in \Omega} \Pr[X = x] \Pr[Y = x]   & \text{(Independency)}      \\
               =& \sum_{x \in \Omega} \Pr[X = x]^2                                         \\
\end{align*}

\begin{lemma}[Collision bound] \label{lem:colbound}
    Let $X$ be a random variable such that its collision probability is upper bounded by the following function of some positive value $\varepsilon$ arbitrarily close to $0$:
    \[
        Col(X) \leq \frac{1 + 4\varepsilon^2}{|\Omega|}
    \]
    Then $\sdtu(X) \leq \varepsilon$, meaning that $X$ is almost uniform over $\Omega$.
\end{lemma}


\begin{proof}
    By definition of statistical distance:
    \[
        \sdtu(X) = \half \sum_{x \in \Omega} \left| \Pr[X = x] - \oneover{|\Omega|} \right|
    \]

    Decompose each of the above addends into the couple $q_x$ and $s_x$:
    \[
        q_x = \Pr[X = x] - \oneover{|\Omega|} \qquad s_x =
        \begin{cases}
            1  & \text{if $q_x \geq 0$} \\
            -1 & \text{otherwise}
        \end{cases}
    \]
    
    Then, by the Euclidean variant of the Cauchy-Schwarz inequality:
    \begin{align*}
        \sdtu(X) =&\ \half \sum_{x \in \Omega} q_x s_x                                                                & \text{(Decomposition)}     \\
              \leq&\ \half \sqrt{ \left( \sum_{x \in \Omega} q_x^2 \right) \left( \sum_{x \in \Omega} s_x^2 \right) } & \text{(Cauchy-Schwarz)}    \\
                 =&\ \half \sqrt{|\Omega| \sum_{x \in \Omega} q_x^2}                                                  & (\forall x \; (s_x^2 = 1)) \\
    \end{align*}

    Focusing on the sum $\sum_{x \in \Omega} q_x^2$:
    \begin{align*}
        \sum_{y \in \Omega} q_x^2 =&\ \sum_{x \in \Omega} \left( \Pr[X = x] - \oneover{|\Omega|} \right)^2                                   &                        \\
                                  =&\ \sum_{x \in \Omega} \left( \Pr[X = x]^2 - \frac{2}{|\Omega|} \Pr[X = x] + \oneover{|\Omega|^2} \right) &                        \\
                                  =&\ Col(X) - \frac{2}{|\Omega|} + \frac{1}{|\Omega|}                                                       &                        \\
                               \leq&\ \frac{1 + 4 \varepsilon^2}{|\Omega|} - \frac{2}{|\Omega|} + \oneover{|\Omega|}                         & \text{(By hypothesis)} \\
                                  =&\ \frac{4 \varepsilon^2}{|\Omega|}                                                                       &                        \\
    \end{align*}

    Thus, getting back to the statistical distance evaluation:
    \begin{align*}
        \sdtu(X) \leq&\ \half \sqrt{|\Omega| \sum_{x \in \Omega} q_x^2}        \\
                 \leq&\ \half \sqrt{|\Omega| \frac{4 \varepsilon^2}{|\Omega|}} \\
                    =&\ \half 2 \varepsilon = \varepsilon                      \\
    \end{align*}

    and by stating that $\varepsilon$ is arbitrarily close to zero, $X$ is statistically close to the uniform distribution over $\Omega$.
\end{proof}

\subsubsection{Leftover hash lemma}

This lemma has been proven by Russell Impagliazzo, Leonid Levin and Michael Luby:

\begin{lemma}[Leftover hash]
    Let $h_s$ be a pairwise-independent hash function with uniform seed $S$, and $\ext \in \binary^d \times \binary^n \to \binary^l$ be a seeded randomness extractor. Then if $\ext(S, x) = h_S(x)$, and $x$ is governed by a random variable $X$ with min-entropy $H_{\infty}(X) \geq k$, where:
    \[
       k = l - 2 \log_2 \varepsilon - 2
    \]
    then $\ext$ is a ($k$, $\varepsilon$)-seeded randomness extractor.
\end{lemma}

\begin{proof}
    The extractor definition requires that:
    \[
        \statdist(Y, (S, U_l)) \leq \varepsilon
    \]
    where $U_l$ distributes unifromly over $\binary^l$. Although the appearance of $S$ in both operands may seem to complicate things, the distance formulation can be changed to remove such constraint, and become simpler:
    \begin{align*}
         &\ \statdist((S, h_S(X))), (S, U_l))                                                                                               &                \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \Pr[(S, U_l) = (s, t)] \right|                      &                \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \oneover{|\binary|^d} \oneover{|\binary^l|} \right| & (S \indep U_l) \\
        =&\ \half \sum_{(s, t) \in \binary^{d + l}} \left| \Pr[(S, h_S(X))) = (s, t)] - \Pr[U_{d + l} = (s, t)] \right|                     &                \\
         &                                                                                          & \mathllap{(U_{d + l} \sim \unifdist(\binary^{d + l}))} \\
        =&\ \statdist((S, h_S(X)), U_{d + l})                                                                                               &                \\
        =&\ \sdtu((S, h_S(X)))                                                                                                              &                \\
    \end{align*}
    
    At this point the \hyperref[lem:colbound]{collision bound} can be used to put an upper bound on this statistical distance. To this end, let $Y = (S, h_S(X))$ and $Y = (S, h_S(X))$ be two \iid{} random variables; the collision probability of $Y$ equates to:
    \begin{align*}
         &\ Col(Y)                                               &                                               \\
        =&\ \Pr[Y = Y']                                          &                                               \\
        =&\ \Pr[S = S' \wedge h_S(X) = h_{S'}(X')]               & \mathllap{\text{(By definition)}}             \\
        =&\ \Pr[S = S'] \Pr[h_S(X) = h_{S'}(X') \knowing S = S'] & \mathllap{\text{(Conditional prob.)}}         \\
        =&\ \Pr[S = S'] \Pr[h_S(X) = h_S(X')]                    & \mathllap{\text{(Condition collapse)}}        \\
        =&\ 2^{-d} \Pr[h_S(X) = h_S(X')]                         & \mathllap{\text{(Uniform collision prob.)}}   \\
        =&\ 2^{-d} \left( \Pr[h_S(X) = h_S(X') \wedge X = X'] + \Pr[h_S(X) = h_S(X') \wedge X \neq X'] \right) & \\
         &                                                       & \mathllap{\text{(Total probability)}}         \\
    \end{align*}

    The two addends are tackled separately. For the first one: 

    \begin{align*}
         &\ \Pr[h_S(X) = h_S(X') \wedge X = X']               &                               \\
        =&\ \Pr[X = X'] \Pr[h_S(X) = h_S(X') \knowing X = X'] & \text{(Conditional prob.)}    \\
        =&\ Col(X) \Pr[h_S(X) = h_S(X') \knowing X = X']      & \text{(Collision def.)}       \\
        =&\ Col(X) \Pr[h_S(X) = h_S(X)]                       & \text{(Condition collapse)}   \\
        =&\ Col(X)                                            & \text{(Prob. of a tautology)} \\
    \end{align*}

    \begin{proposition}
        Given any random variable $X$:
        \[
            H_\infty(X) \geq k \implies Col(X) \leq 2^{-k} \qedhere
        \]
    \end{proposition}

    \begin{proof}
        By definition of min-entropy\footnote{The definition specifies explicitly a base-2 logarithm, but can actually be any base; in case, the statement to be proved shall correct the collision probability bound accordingly by changing its exponential basis}:
        \begin{align*}
            -\log \max_{x \in \Omega}(\Pr [X = x]) &\geq k      \\
            \log \max_{x \in \Omega}(\Pr [X = x])  &\leq -k     \\
            \max_{x \in \Omega}(\Pr [X = x])       &\leq 2^{-k} \\
        \end{align*}

        On the other hand:
        \begin{align*}
            Col(X) =&\ \sum_{x \in \Omega} \Pr[X = x]^2                               \\
                \leq&\ \sum_{x \in \Omega} \Pr[X = x] \max_{y \in \Omega}(\Pr[X = y]) \\
                   =&\ \max_{y \in \Omega}(\Pr[X = y]) \sum_{x \in \Omega} \Pr[X = x] \\
                   =&\ \max_{y \in \Omega}(\Pr[X = y])                                \\
        \end{align*}

        Therefore:
        \[
            Col(X) \leq \max_{x \in \Omega}(\Pr[X = x]) \leq 2^{-k}
        \]
    \end{proof}

    Shifting focus to the second addend:

    \begin{align*}
            &\ \Pr[h_S(X) = h_S(X') \wedge X \neq X']                  &                                                  \\
           =&\ \Pr[X \neq X'] \Pr[h_S(X) = h_S(X') \knowing X \neq X'] & \text{(Conditional prob.)}                       \\
        \leq&\ \Pr[h_S(X) = h_S(X') \knowing X \neq X']                & \mathllap{\text{(Prob. is not greater than 1)}}  \\
    \end{align*}

    \begin{proposition}
        If $h_S$ is a pairwise independent hash function, then  $\Pr[h_S(X) = h_S(X') \knowing X \neq X'] \leq 2^{-l}$
    \end{proposition}
    
    \begin{proof}
        \todo{Left as exercise.}
    \end{proof}

    Going back to the collision probability of $Y$, and using the two propositions above:
    \begin{align*}
        Col(Y) \leq&\ 2^{-d} \left( Col(X) + \Pr[h_S(X) = h_S(X') \knowing X \neq X'] \right) & \\
               \leq&\ 2^{-d} (2^{-k} + 2^{-l}) & \\
                  =&\ 2^{-d - l} (2^{-k + l} + 2^{-l + l}) & \\
                  =&\ 2^{-d - l} (2^{-(l - 2 \log_2(\varepsilon) - 2) + l} + 1)  & \\
                  =&\ 2^{-d - l} (2^{2 \log_2(\varepsilon) + 2} + 1)  & \\
                  =&\ 2^{-d - l} (4\varepsilon^2 + 1) & \\
    \end{align*}

    Applying the collision bound entails that $\sdtu(Y) \leq \varepsilon$, therefore $h_S$ is a $(k, \varepsilon)$-seeded randomness extractor.
\end{proof}

Note that for smaller values of $\varepsilon$ we have greater values of min-entropy. A problem that is still open is to find an extractor with $k \approx l$.
